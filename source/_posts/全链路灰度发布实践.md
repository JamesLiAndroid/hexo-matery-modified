---
title: 全链路灰度发布实践
top: false
cover: false
toc: true
mathjax: true
date: 2021-03-06 13:24:53
password:
summary:
tags:
categories:
---

# 全链路灰度发布方案

## 前言

根据目前公司现状，需要进行上云的操作，涉及到服务保障的部分，要求做到三个9的可用率，所以率先进行灰度发布的预研，保证发布过程中进行平滑升级的操作，及早暴露发布问题，做到尽可能的无损发布。

## 方案综述

目前平台这边整个技术栈后端以java为主，基于Spring Cloud体系搭建，所以在灰度发布时，也优先考虑java侧的技术选型，最终选择使用Nepxion Discovery框架来完成这个操作。该框架支持从前端到后端的完整发布体系，可以通过header、cookie或者params实现从前端来控制后端发布访问同类服务不同版本的能力，做到动态控制整个访问路径。

针对前端技术栈以Vue框架为主，部署在nginx，利用nginx+consul+upsync体系实现前端的灰度发布，实质上模拟了新服务的上线引流，旧服务的逐步下线。通过nginx进行负载均衡，调整流量的内容，使用consul进行发布信息的存储，主要是存储要访问的url信息以及访问配置策略，最后通过upsync(nginx-upsync-module)插件从consul中拉取配置信息，实现不需要重启nginx就可以实时变更nginx配置信息。通过组合的方式实现前端的灰度发布。

由于是进行方案验证阶段，仅以最小系统进行验证，选定omp相关服务进行验证，后端服务包括：

- gateway
- base-opt
- uaa
- umc
- dict
- id-generator

对于后端服务，只有gateway是单独部署的，其余服务均部署两个相同的镜像服务。在本次验证中，主要验证登录接口获取access_token信息，通过变更镜像服务的版本来实现不同版本的访问。

前端服务只有omp的前端项目（test-frontend1.0-ui），在两台机器上各部署一个服务，然后前置一个nginx进行反向代理，对访问nginx的ip地址进行筛选，确定要访问的不同的前端服务。针对不同的前端服务，设置不同的header信息，网关侧根据拦截到的header信息确定访问后端的服务的版本，进而实现访问链路的控制。

最终要实现的是，通过访问不同的前端，进而确定访问不通版本的微服务，实现全链路的灰度发布体系。

## 资源准备

这里一共准备了三台主机

- 192.168.59.103：后端灰度发布环境1.0版本服务
- 192.168.59.104：后端灰度发布环境，uaa1.1版本，其它服务1.0版本
- 192.168.59.105：前端灰度发布环境

## 整体架构部署图

![](gateway灰度流程.png)

![](前端灰度流程1.png)

## 后端发布方案

基于[Nepxion Discovery](https://github.com/Nepxion/Discovery)进行后端灰度发布的操作，以常用的登录接口为例子进行灰度发布的测试。首先采用硬编码配置的方式，手动实现流量的切换，然后使用前端访问的方式，通过header中配置变量，传递给网关再进行后向的服务访问。

首先简要介绍下Nepxion Discovery，同时支持Spring Cloud体系和Spring Cloud Alibaba体系，拥有广泛的框架层面支持。在执行灰度发布的时候，建立了5个层面的规则：

- 版本匹配（version）
- 区域匹配（region）
- 环境匹配（env）
- 分组匹配（group），我们利用分组来拉取对应的配置文件信息
- 可用区匹配（zone）

除了这五个层面的匹配规则，剩下的还支持：

- 权重设定
- ip地址黑名单屏蔽
- header参数、param参数、Cookie参数传递
- 条件表达式的解析
- 全链路传递，通过全链路参数传递能够保证把前端项目也能纳入到整个体系

等等。。。

整个服务治理架构图如下：

![](Nepxion服务治理.png)

说起来简单，但是使用起来非常复杂，尤其是超级多的配置项非常容易把人整晕了，所以还得好好查看文档！

### 1. 网关侧控制灰度发布

首先测试网关侧进行灰度发布的设定，对整个体系要进行改造，对包含在内的所有服务--gateway、uaa、umc、base-opt、id-gen、dict这六个项目进行改造，对其创建新的分支*feature.gray_deploy.nepxion*，进行修改。

这里主要演示通过版本信息进行灰度发布的内容，对登录接口进行灰度发布，上述提到的uaa、umc、base-opt、id-gen、dict这几个服务均部署两套，部署单个gateway服务，然后针对uaa服务添加1.0版本和1.1版本，利用postman进行接口测试，通过查看日志来判断登录到不同版本的服务。

#### 1.1 灰度发布配置信息

首先对nacos重新添加namespace为zgray-deploy-test，其签名为14f13d68-b0d8-44e6-9e06-1e0b4bbbab7c。将对应服务的配置文件均添加到该namespace下，包括服务也要注册到这里面。

然后确定服务的分组信息为gray_deploy_group，将灰度发布的配置文件的名称确定为gray_deploy_group，灰度发布配置信息如下：

```xml

<?xml version="1.0" encoding="UTF-8"?>
<rule>
    <strategy>
        <version>{"test-uaa":"1.0","test-umc":"1.0","test-id-generator":"1.0","test-dict":"1.0","test-base-opt":"1.0"}</version>
    </strategy>
</rule>

```

这样配置以后，从gateway开始调用后向各个服务的版本为1.0，如果需要变更版本，直接修改配置文件即可。作为整个服务链路上的服务，将会根据配置分组信息（group），来确定拉取的灰度发布配置信息。

最后在灰度发布操作时，修改test-uaa的版本为1.1，重新发布该配置信息，则可以使调用链路发生改变。

#### 1.2 微服务源码改造部分

针对微服务源码，主要包含引入依赖项，配置信息变更，启动参数变更的内容。

##### 1.1.1 引入依赖项

在项目的pom.xml中引入依赖信息，这里要注意的是，如果该项目下拥有多个module，将依赖信息一定要放到对应服务的module中的pom.xml文件下，否则容易导致在jenkins构建的时候找不到依赖信息而报错。

对pom.xml文件修改如下：

```xml

    <properties>
        <discovery.version>6.6.0</discovery.version>
    </properties>

<!-- 灰度发布依赖信息 -->
        <dependency>
            <groupId>com.nepxion</groupId>
            <artifactId>discovery-plugin-register-center-starter-nacos</artifactId>
            <version>${discovery.version}</version>
        </dependency>
        <dependency>
            <groupId>com.nepxion</groupId>
            <artifactId>discovery-plugin-config-center-starter-nacos</artifactId>
            <version>${discovery.version}</version>
        </dependency>
        <dependency>
            <groupId>com.nepxion</groupId>
            <artifactId>discovery-plugin-admin-center-starter</artifactId>
            <version>${discovery.version}</version>
        </dependency>
        <dependency>
            <groupId>com.nepxion</groupId>
            <artifactId>discovery-plugin-strategy-starter-gateway</artifactId>
            <version>${discovery.version}</version>
            <!-- 如果出现构建冲突问题，遇到下面的错误，请剔除下面exclusions中的依赖信息  -->
            <!--
***************************
APPLICATION FAILED TO START
***************************

Description:

Parameter 0 of method modifyRequestBodyGatewayFilterFactory in org.springframework.cloud.gateway.config.GatewayAutoConfiguration required a bean of type 'org.springframework.http.codec.ServerCodecConfigurer' that could not be found.

The injection point has the following annotations:
        - @org.springframework.beans.factory.annotation.Autowired(required=true)


Action:

Consider defining a bean of type 'org.springframework.http.codec.ServerCodecConfigurer' in your configuration.
 -->
            <!-- <exclusions>
            <exclusion>
                <groupId>org.springframework.boot</groupId>
                <artifactId>spring-boot-starter-web</artifactId>
            </exclusion>
                <exclusion>
                    <groupId>org.springframework.boot</groupId>
                    <artifactId>spring-boot-starter-webflux</artifactId>
                </exclusion>
        </exclusions> -->
        </dependency>
        <dependency>
            <groupId>com.nepxion</groupId>
            <artifactId>discovery-plugin-strategy-starter-skywalking</artifactId>
            <version>${discovery.version}</version>
        </dependency>

```

在添加完成后，一定进行本地的构建验证。验证通过后，进行后续操作。

##### 1.1.2 配置信息变更

对每一个服务的*bootstrap.yaml*配置文件进行修改，达成变量信息可配置的能力。

```yaml

spring:
  ......
  cloud:
    nacos:
      ......
      discovery:
        ......
        metadata:
          group: ${gray_deploy_group:discovery-guide-group} # 分组信息，该处group信息和nacos中服务的配置文件所在group要区分
          version: ${gray_deploy_version:1.0} # 版本
          region: ${gray_deploy_region:dev} # 区域，dev、test、release等
          env: ${gray_deploy_env:env1} # 环境
          zone: ${gray_deploy_zone:zone1} # 可用区，云服务器所在的可用区
# config rule加载的配置，需要单独进行配置
nacos:
  server-addr: ${nacos_ip:192.168.66.206:38848}
  plugin:
    namespace: ${nacos_namespace:858b37b1-35be-4564-a24e-dc2c322d5784}

```

这个配置内容理论上也可以放在具体服务的配置文件中，但是这里进行测试后发现，放在启动项的配置文件中更合适。

随后要区分网关和其它服务的配置，网关侧配置如下：

```yaml

spring:
  application:
    strategy:
      gateway:
        core:
          header:
            transmission:
              enabled: false
        header:
          # 当外界传值Header的时候，服务也设置并传递同名的Header，需要决定哪个Header传递到后边的服务去，该开关依赖前置过滤器的开关。如果下面开关为true，以服务设置为优先，否则以外界传值为优先。缺失则默认为true
          priority: true
        original:
          header:
            ignored: false

```

其它服务配置信息如下：

```yaml

spring:
  application:
    strategy:
      service:
        header:
          # 当外界传值Header的时候，服务也设置并传递同名的Header，需要决定哪个Header传递到后边的服务去，该开关依赖前置过滤器的开关。如果下面开关为true，以服务设置为优先，否则以外界传值为优先。缺失则默认为true
          priority: true

```

这里需要注意的是，header.priority以及service.priority均设置为true，在这个位置，只进行后端发布的验证。如果是前端加入灰度发布的时候，这两个参数要设置为false。

最后要切换一下配置信息中的admin服务的连接地址。

##### 1.1.3 启动参数变更

这里的启动参数主要是指Dockerfile中的启动命令，主要添加了Nepxion的启动项，把下面的内容接入到Dockerfile中：

```txt

-Dgray_deploy_group=${GRAY_DEPLOY_GROUP} -Dgray_deploy_version=${GRAY_DEPLOY_VERSION} -Dgray_deploy_region=${GRAY_DEPLOY_REGION} -Dgray_deploy_env=${GRAY_DEPLOY_ENV} -Dgray_deploy_zone=${GRAY_DEPLOY_ZONE} 

```

以gateway为例，完整的Dockerfile信息如下：

```Dockerfile

# 以基础镜像为底，执行
FROM 192.168.93.105/test/base_backend:0.0.2

# 安装字体库文件，解决验证码问题
RUN sed -i 's/dl-cdn.alpinelinux.org/mirrors.tuna.tsinghua.edu.cn/g' /etc/apk/repositories &&\
    apk add --no-cache ttf-dejavu fontconfig

COPY ./target/test-gateway-1.1.0-SNAPSHOT.jar /app

CMD ["sh", "-c", "java -XX:MaxRAMPercentage=85.0 -XX:MaxRAM=2048m -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap -XX:MaxRAMFraction=8 -javaagent:/app/skywalking-agent/skywalking-agent.jar -Dskywalking.agent.namespace=${SKYWALKING_NAMESPACE} -Dskywalking.agent.service_name=${SKYWALKING_TARGET_SERVICE_NAME} -Dskywalking.collector.backend_service=${SKYWALKING_IP_PORT}  -Dspring.profiles.active=${CHANNEL} -Dspring.cloud.client.ip-address=${IP_ADDR}  -Dnacos_ip=${NACOS_IP} -Dnacos_namespace=${NACOS_NAMESPACE} -Dgray_deploy_group=${GRAY_DEPLOY_GROUP} -Dgray_deploy_version=${GRAY_DEPLOY_VERSION} -Dgray_deploy_region=${GRAY_DEPLOY_REGION} -Dgray_deploy_env=${GRAY_DEPLOY_ENV} -Dgray_deploy_zone=${GRAY_DEPLOY_ZONE} -jar test-gateway-1.1.0-SNAPSHOT.jar"]

```

到此，所有的微服务就改造完成了。下面进行构建任务的改造。

#### 1.3 构建任务改造

所有要改造的构建任务如图：

![](构建任务截图.png)

针对构建任务进行下面的配置：

- 添加参数信息，主要是添加灰度发布的参数，参数列表如下：

| 参数名称 | 默认内容 | 参数说明 |
|---|---|---|
|  DEPLOY_TO_SELF  | 0  | 判断是否在该机器上部署，0为第一台机器、1为第二台机器，2为第三台机器，在shell脚本中判断是否继续执行  |
|  GRAY_DEPLOY_GROUP  |  gray_deploy_group |  灰度发布，分组信息 |
|  GRAY_DEPLOY_VERSION  | 1.0  |  灰度发布，版本信息 |
|  GRAY_DEPLOY_REGION  |  dev |  灰度发布，区域，dev、test、release等 |
| GRAY_DEPLOY_ENV | env1 | 灰度发布，部署环境信息 |

- 更换git的branch信息为*/feature.gray-deploy.nepxion
- 更换镜像名称，添加*gray-deploy-*的内容
- 更改发布的主机内容，这里包含两台主机，192.168.59.103和192.168.59.104，模拟两个环境
- 修改启动脚本信息，添加灰度发布的内容，修改配置的nacos中的namespace信息如下：

```shell

#--------------------------------------------------------------------------

if [ ${DEPLOY_TO_SELF} == '0' ]; then
    #--------------------------------------------------------------------------

    # 判断是否存在镜像
    # docker ps -a | grep -w test-data-dict-gray-deploy-develop &> /dev/null
    # 如果存在先停止运行并删除镜像
    if [ "$(docker ps -a | grep -w test-data-dict-gray-deploy-develop)" ]; then
        echo "test-data-dict-gray-deploy-develop is exsited!!"
        docker stop `docker ps -a | grep -w test-data-dict-gray-deploy-develop | awk '{print $1}'`
        docker rm `docker ps -a | grep -w test-data-dict-gray-deploy-develop | awk '{print $1}'`
        docker image rm `docker images | grep -w test-data-dict-gray-deploy-develop | awk '{print $3}'`
    fi

    echo "pull the test-data-dict-gray-deploy-develop image"
    docker pull 192.168.93.105/test/test-data-dict-gray-deploy-develop:$BUILD_NUMBER
    docker run -d -p 19090:19090 -e CHANNEL="standalone" -e IP_ADDR="192.168.59.103" -e NACOS_IP="192.168.66.206:38848" -e NACOS_NAMESPACE="14f13d68-b0d8-44e6-9e06-1e0b4bbbab7c"  -e SKYWALKING_NAMESPACE="test-dev" -e SKYWALKING_TARGET_SERVICE_NAME="test-data-dict-develop-2" -e SKYWALKING_IP_PORT="192.168.66.208:11800" -e GRAY_DEPLOY_ZONE=${GRAY_DEPLOY_ZONE} -e GRAY_DEPLOY_ENV=${GRAY_DEPLOY_ENV} -e GRAY_DEPLOY_REGION=${GRAY_DEPLOY_REGION} -e GRAY_DEPLOY_VERSION=${GRAY_DEPLOY_VERSION}  -e GRAY_DEPLOY_GROUP=${GRAY_DEPLOY_GROUP} --name test-dict-${DEPLOY_TO_SELF} 192.168.93.105/test/test-data-dict-gray-deploy-develop:$BUILD_NUMBER

else
    echo "=================================不在该机器进行部署==============================================="

fi

```

在jenkins构建时，通过修改DEPLOY_TO_SELF参数决定部署到哪台机器上，然后对GRAY_DEPLOY_GROUP统一使用gray_deploy_group名称，拉取该分组下的配置文件，注入到服务中。

各个微服务根据注入到服务中的灰度发布配置选择是否接受网关指定的流量，如果不接受则不对调用请求作出响应。

#### 1.4 微服务部署以及上线监控

手动点击jenkins的构建任务，DEPLOY_TO_SELF设为0时统一执行一遍构建，DEPLOY_TO_SELF设为1时再统一执行一遍构建。特别针对uaa服务，在DEPLOY_TO_SELF设为1时，将版本号GRAY_DEPLOY_VERSION设为1.1，所有的服务构建完成后，都将注册到nacos集群中，服务列表如下：

![](服务列表信息.png)

服务的metadata信息如下，以uaa服务为例子：

![](uaa服务metadata信息.png)

最后是admin中的服务面板信息，如下

![](服务面板信息.png)

这样所有的服务上线并正常运行了。下面开始简单的进行测试，使用postman进行模拟登录请求的操作。

#### 1.5 示例1：简单的灰度发布

首先使用默认的灰度发布配置，进行请求，通过查看两个不同版本的uaa服务的日志，确定1.0版本的uaa承接了该登录请求。

然后打开1.1版本的uaa服务的日志信息，修改灰度发布的配置如下：

```xml

<?xml version="1.0" encoding="UTF-8"?>
<rule>
    <strategy>
        <version>{"test-uaa":"1.1","test-umc":"1.0","test-id-generator":"1.0","test-dict":"1.0","test-base-opt":"1.0"}</version>
    </strategy>
</rule>


```

uaa服务的版本被修改为1.1版本，这时候继续进行请求，可以看到1.1版本的uaa服务承接了该登录请求。

这样初步实现了简单的灰度发布。

#### 1.6 示例2：通过header参数进行蓝绿部署

首先对各个服务的配置信息进行修改，对gateway的配置修改如下：

```yaml

spring:
  application:
    strategy:
      gateway:
        header:
          # 当外界传值Header的时候，服务也设置并传递同名的Header，需要决定哪个Header传递到后边的服务去，该开关依赖前置过滤器的开关。如果下面开关为true，以服务设置为优先，否则以外界传值为优先。缺失则默认为true
          priority: false

```

对其它服务配置修改如下：

```yaml

spring:
  application:
    strategy:
      service:
        header:
          # 当外界传值Header的时候，服务也设置并传递同名的Header，需要决定哪个Header传递到后边的服务去，该开关依赖前置过滤器的开关。如果下面开关为true，以服务设置为优先，否则以外界传值为优先。缺失则默认为true
          priority: false

```

这样修改后，就可以进行全链路的参数传递了，根据传递的header参数，可以匹配灰度发布的规则。

然后对灰度发布的配置文件进行修改，如下：

```xml

<?xml version="1.0" encoding="UTF-8"?>
<rule>
    <strategy>
    <version>{"test-uaa":"1.1","test-umc":"1.0","test-id-generator":"1.0","test-dict":"1.0","test-base-opt":"1.0"}</version>
    </strategy>

    <strategy-customization>
        <conditions type="blue-green">
            <condition id="blue-condition" expression="#H['a'] == '1'" version-id="blue-version-route"/>
            <condition id="green-condition" expression="#H['a'] == '2'" version-id="green-version-route"/>
        </conditions>

        <routes>
            <route id="blue-version-route" type="version">{"test-uaa":"1.1","test-umc":"1.0","test-id-generator":"1.0","test-dict":"1.0","test-base-opt":"1.0"}</route>
            <route id="green-version-route" type="version">{"test-uaa":"1.0","test-umc":"1.0","test-id-generator":"1.0","test-dict":"1.0","test-base-opt":"1.0"}</route>
        </routes>
    </strategy-customization>
</rule>

```

对灰度发布的内容进行了筛选，默认最上面的strategy为兜底策略，在下面strategy-customization为蓝绿发布的策略，根据请求中的header参数信息为条件进行删选，匹配不同的版本链路信息。

最后在postman的请求中，添加header参数为a，变更a的值分别为空，1,2，可以查看运行结果。整体运行图如下：

![](蓝绿部署示意图.png)

## 后端发布问题

1. 流量控制后，用户数据兜底问题，就是如何控制发布时出现问题的用户进行数据回滚的操作。
2. 未能探索出将灰度发布的链路信息写入skywalking的方式
3. 资源问题，两套环境的维护
4. jenkins构建项目的规划，目前一个项目构建两个版本，不太用的过来
5. k8s支持，如果以现有框架继续运行可以使用，如果将基础设施落到k8s或者istio上，就无法再使用该框架，例如将nacos和gateway均沉淀到k8s中
6. 如果存在较多的小版本的相同服务，如何确定灰度时的版本？默认的灰度发布的版本是什么？
7. 如果未指定默认版本时，也就是配置文件不存在时，应该怎么去设置？

## 前端发布方案

针对前端的特点，使用nginx进行前端服务的反向代理，部署两套前端服务，仅仅配置在网络请求时的header信息不同。在这两套前端服务之前，使用openresty进行反向代理，外加使用nginx的插件upsync进行动态变更前端地址，通过upsync连接consul配置中心，获取前端地址。最后使用redis存储ip地址信息，利用openresty中的lua模块，读取redis中缓存的ip地址信息，再根据ip地址的匹配，决定要访问的前端地址信息。整体架构包括openresty+lua+consul+upsync+redis组件构成整个前端灰度发布体系。

### 2. 前端控制灰度发布

#### 2.1 前端项目改造

针对前端项目的改造，使用omp项目的前端，在request.js中配置发送的header信息，如下：

```js

// request interceptor
service.interceptors.request.use(config => {
    //添加平台标识
    config.headers['identification'] = identification
        //在token中添加市区
    config.headers['timezone'] = -(new Date().getTimezoneOffset() / 60)
        //判断token是否过期，过期则携带刷新token重新请求
        // const refreshToken = Vue.ls.get(REFRESH_TOKEN)
        // if(refreshToken){
        //   judgeTokenOutTime()
        // }
        //多页面的时候会找不到ls所以需要进行判断
    // ======发送header请求内容======
    // 修改这个内容
    config.headers['a']="1"
    
    let token;
    if (Vue.ls) {
        token = Vue.ls.get(ACCESS_TOKEN)
    } else {
        if (window.localStorage.getItem('pro__Access-Token')) {
            token = JSON.parse(window.localStorage.getItem('pro__Access-Token')).value
        }
    }

    config.headers['Authorization'] = getBase64Code(grantTypeUsername, grantTypePassword) //请求携带authorization
    if (token) {
        config.headers['Access-Token'] = token // 让每个请求携带自定义 token 请根据实际情况自行修改

        config.headers['Authorization'] = 'Bearer ' + token
    }
    //判断是不是登出请求
    // if(config.data === 'logout'){
    // config.headers['Authorization'] = token
    // }
    return config
}, err => {

})


```

在不同版本的前端服务中，修改header信息，当a=1的时候，访问1.1版本的uaa服务；当a=2的时候，访问版本为1.0的uaa服务。

最后将这两个前端项目分别部署到192.168.59.103、192.168.59.104的服务器上。

#### 2.2 openresty+lua+upsync+redis环境搭建

##### 2.2.1 安装luajit

```

$ mkdir luajit

$ wget https://github.com/openresty/luajit2/archive/v2.1-20201027.tar.gz

$ tar -zxvf v2.1-20201027.tar.gz

$ cd luajit2-2.1-20201027/

// 编译安装lua环境
$ make linux PREFIX=/opt/luajit

$ sudo mkdir -p /opt/luajit && sudo make install PREFIX=/opt/luajit

// 配置环境变量
$ sudo vim /etc/profile
// 在文件末尾追加
# lua
export LUAJIT_BIN=/opt/luajit/bin/
export LUAJIT_LIB=/opt/luajit/lib/
export LUAJIT_INC=/opt/luajit/include/luajit-2.1/
export PATH=$LUAJIT_BIN:$PATH
export PATH=$LUAJIT_LIB:$PATH
export PATH=$LUAJIT_INC:$PATH

// :wq保存退出

// 配置so到配置文件中
$ sudo echo "/opt/luajit/lib/" >> /etc/ld.so.conf

// 配置生效
$ sudo ldconfig

// 或者使用短连接的方式
$ sudo ln -s /opt/luajit/lib/libluajit-5.1.so.2 /usr/lib64/

// 测试lua
$ luajit
LuaJIT 2.1.0-beta3 -- Copyright (C) 2005-2020 Mike Pall. https://luajit.org/
JIT: ON SSE3 SSE4.1 BMI2 fold cse dce fwd dse narrow loop abc sink fuse
>

```

##### 2.2.2 编译安装openresty

```
// 安装依赖信息
$ sudo yum install pcre-devel openssl-devel gcc curl
$ sudo yum install -y yum-utils

$ sudo yum -y install libpcre3 libpcre3-dev ruby zlib1g-dev patch

$ wget https://openresty.org/download/openresty-1.17.8.2.tar.gz

$ tar -zxvf openresty-1.17.8.2.tar.gz

// 下载upsync插件
$ mkdir /home/centos/nginx-modules

$ wget https://github.com//weibocom/nginx-upsync-module/archive/v2.1.3.tar.gz

$ tar -zxvf v2.1.3.tar.gz

$ mv nginx-upsync-module-v2.1.3 nginx-upsync-module

// 下载nginx_upstream_check_module插件
$ git clone https://github.com/xiaokai-wang/nginx_upstream_check_module.git

$ cd openresty-1.17.8.2

// 对openresty添加patch
$ patch -p1 < /home/centos/nginx-modules/nginx_upstream_check_module/check_1.12.1+.patch

// 编译并安装openresty
$ ./configure --add-module=/home/centos/nginx-modules/nginx-upsync-module --add-module=/home/centos/nginx-modules/nginx_upstream_check_module --prefix=/opt/openresty --with-http_stub_status_module --with-http_ssl_module --with-http_flv_module --with-http_gzip_static_module --with-http_realip_module

$ gmake && sudo gmake install

// 安装完成后进行启动
$ sudo /opt/openresty/nginx/sbin/nginx

// 访问nginx
$ curl localhost

```

##### 2.2.3 安装consul

```

// 下载consul组件
$ cd ~ && wget https://releases.hashicorp.com/consul/1.7.3/consul_1.7.3_linux_amd64.zip

$ unzip consul_1.7.3_linux_amd64.zip

$ cd consul_1.7.3_linux_amd64/

$ sudo mkdir -p /opt/consul && sudo cp -r ./* /opt/consul

$ sudo mkdir -p /opt/consul/node1 /opt/consul/node2 /opt/consul/node3

$ sudo mkdir -p /opt/consul/node1/logs /opt/consul/node2/logs /opt/consul/node3/logs

$ sudo vim /opt/consul/node1

// 添加第一个节点的配置信息
{
  "datacenter": "nginx-consul",
  "data_dir": "/opt/consul/data/node1",
  "log_file": "/opt/consul/data/node1/consul.log",
  "log_level": "INFO",
  "server": true,
  "node_name": "node1",
  "ui": true,
  "bind_addr": "192.168.59.105",
  "client_addr": "192.168.59.105",
  "advertise_addr": "192.168.59.105",
  "bootstrap_expect": 3,
  "ports":{
    "http": 8510,
    "dns": 8610,
    "server": 8310,
    "serf_lan": 8311,
    "serf_wan": 8312
   }
}

// :wq保存退出

$ sudo vim /opt/consul/node2

// 添加第一个节点的配置信息
{
  "datacenter": "nginx-consul",
  "data_dir": "/opt/consul/data/node2",
  "log_file": "/opt/consul/data/node2/consul.log",
  "log_level": "INFO",
  "server": true,
  "node_name": "node2",
  "ui": true,
  "bind_addr": "192.168.59.105",
  "client_addr": "192.168.59.105",
  "advertise_addr": "192.168.59.105",
  "bootstrap_expect": 3,
  "ports":{
    "http": 8520,
    "dns": 8620,
    "server": 8320,
    "serf_lan": 8321,
    "serf_wan": 8322
    }
}


// :wq保存退出

$ sudo vim /opt/consul/node3

// 添加第一个节点的配置信息
{
  "datacenter": "nginx-consul",
  "data_dir": "/opt/consul/data/node3",
  "log_file": "/opt/consul/data/node3/consul.log",
  "log_level": "INFO",
  "server": true,
  "node_name": "node3",
  "ui": true,
  "bind_addr": "192.168.59.105",
  "client_addr": "192.168.59.105",
  "advertise_addr": "192.168.59.105",
  "bootstrap_expect": 3,
  "ports":{
    "http": 8530,
    "dns": 8630,
    "server": 8330,
    "serf_lan": 8331,
    "serf_wan": 8332
    }
}

// :wq保存退出

$ cd /opt/consul && sudo vim consul_start.sh
// 添加以下内容

# !/bin/bash

nohup /opt/consul/consul agent -config-file=/opt/consul/data/node1/consul_conf.json > /dev/null 2>&1 &
sleep 10
nohup /opt/consul/consul agent -config-file=/opt/consul/data/node2/consul_conf.json -retry-join=192.168.59.105:8311 > /dev/null 2>&1 &
sleep 10
nohup /opt/consul/consul agent -config-file=/opt/consul/data/node3/consul_conf.json -retry-join=192.168.59.105:8311 > /dev/null 2>&1 &

// :wq保存退出

// 启动整个consul的伪集群
$ sudo sh +x consul_start.sh

// 启动完成后直接访问
$ curl http://192.168.59.105:8510/

```

##### 2.2.4 安装redis

```
$ sudo yum install redis

$ systemctl enable redis && systemctl start redis

$ redis-cli -h 127.0.0.1 -p 6379
127.0.0.1:6379>

// ctrl+c退出

```

这样整个环境就搭建完成了。

#### 2.3 前端运行策略

##### 2.3.1 前端部署配置

在192.168.59.103、192.168.59.104机器上进行部署前端服务，同上。

##### 2.3.2 openresty nginx配置

```

$ cd /opt/openresty/nginx/conf

$ sudo vim nginx.conf

```

nginx.conf配置修改如下：

```conf


#user  nobody;
worker_processes  1;

#error_log  logs/error.log;
#error_log  logs/error.log  notice;
#error_log  logs/error.log  info;

#pid        logs/nginx.pid;


events {
    worker_connections  1024;
}


http {
    # 导入lua的开发包支持
    lua_package_path "/opt/openresty/lualib/?.lua;;";
    lua_package_cpath "/opt/openresty/lualib/?.so;;";

    #include /opt/openresty/nginx/conf/lua/gray_deploy.conf;

    include       mime.types;
    default_type  application/octet-stream;

    #log_format  main  '$remote_addr - $remote_user [$time_local] "$request" '
    #                  '$status $body_bytes_sent "$http_referer" '
    #                  '"$http_user_agent" "$http_x_forwarded_for"';

    #access_log  logs/access.log  main;

    upstream baseClient {
       #ip_hash;
       # 这里是consul的leader节点的HTTP端点
       upsync 192.168.59.105:8510/v1/kv/upstreams/base/ upsync_timeout=6m upsync_interval=500ms upsync_type=consul strong_dependency=off;
       # consul访问不了的时候的备用配置
       upsync_dump_path /opt/openresty/nginx/conf/app_backup.conf;
       # 这里是为了兼容Nginx的语法检查
       include /opt/openresty/nginx/conf/app_backup.conf;
       # 下面三个配置是健康检查的配置
       check interval=1000 rise=2 fall=2 timeout=3000 type=http default_down=false;
       check_http_send "HEAD / HTTP/1.0\r\n\r\n";
       check_http_expect_alive http_2xx http_3xx;
    }

    upstream grayClient {
       #ip_hash;
       # 这里是consul的leader节点的HTTP端点
       upsync 192.168.59.105:8510/v1/kv/upstreams/gray/ upsync_timeout=6m upsync_interval=500ms upsync_type=consul strong_dependency=off;
       # consul访问不了的时候的备用配置
       upsync_dump_path /opt/openresty/nginx/conf/app_backup.conf;
       # 这里是为了兼容Nginx的语法检查
       include /opt/openresty/nginx/conf/app_backup.conf;
       # 下面三个配置是健康检查的配置
       check interval=1000 rise=2 fall=2 timeout=3000 type=http default_down=false;
       check_http_send "HEAD / HTTP/1.0\r\n\r\n";
       check_http_expect_alive http_2xx http_3xx;
    }


    sendfile        on;
    #tcp_nopush     on;

    #keepalive_timeout  0;
    keepalive_timeout  65;

    #gzip  on;

    server {
        listen       80;
        server_name  localhost;

        #charset koi8-r;

        #access_log  logs/host.access.log  main;

        location / {
            #root   html;
            #index  index.html index.htm;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            # lua脚本处理ip地址的匹配
            content_by_lua_file /opt/openresty/nginx/conf/lua/gray_deploy.lua;
        }

        # 灰度发布之前的路由信息
        location @base{
                proxy_pass http://baseClient;
        }

        # 灰度发布的路由信息
        location @gray{
                proxy_pass http://grayClient;
        }

        # 健康检查 - 查看负载均衡列表
        location /upstream_list {
            upstream_show;
        }

        # 健康检查 - 查看负载均衡的状态
        location /upstream_status {
            check_status;
            access_log off;
        }

        location /lua {
            default_type 'text/html';
            content_by_lua_file conf/lua/test.lua;
        }

        #error_page  404              /404.html;

        # redirect server error pages to the static page /50x.html
        #
        error_page   500 502 503 504  /50x.html;
        location = /50x.html {
            root   html;
        }
    }

}

```

上述配置完成后，需要在命令行执行下测试命令，保证修改nginx.conf文件没有错误。

```
$ sudo /opt/openresty/nginx/sbin/nginx -t
nginx: the configuration file /opt/openresty/nginx/conf/nginx.conf syntax is ok
nginx: configuration file /opt/openresty/nginx/conf/nginx.conf test is successful

```

##### 2.3.3 openresty lua脚本编写

```

$ cd /opt/openresty/nginx/conf && mkdir lua && touch gray_deploy.lua

$ sudo vim lua/gray_deploy.lua
// 编写lua脚本
local redis = require "resty.redis"
local cache = redis.new()
cache:set_timeout(60000)

local ok, err = cache.connect(cache, '127.0.0.1', 6379)
if not ok then
    ngx.say("failed to connect:", err)
    return
end

--local red, err = cache:auth("foobared")
--if not red then
--    ngx.say("failed to authenticate: ", err)
--    return
--end

local local_ip = ngx.req.get_headers()["X-Real-IP"]
if local_ip == nil then
    local_ip = ngx.req.get_headers()["x_forwarded_for"]
end

if local_ip == nil then
    local_ip = ngx.var.remote_addr
end
--ngx.say("local_ip is : ", local_ip)

-- 根据redis中缓存的ip地址确定访问的前端地址
local intercept = cache:get(local_ip)

if intercept == local_ip then
    ngx.exec("@gray")
    return
end

ngx.exec("@base")

local ok, err = cache:close()

if not ok then
    ngx.say("failed to close:", err)
    return
end

// :wq保存退出

```

##### 2.3.4 对consul中设置前端的url地址

在consul中通过curl调用接口的方式添加配置信息，如下：

```

$ curl -X PUT -d '{"weight":1, "max_fails":2, "fail_timeout":10}' http://192.168.59.105:8510/v1/kv/upstreams/base/192.168.59.104:20080

$ curl -X PUT -d '{"weight":1, "max_fails":2, "fail_timeout":10}' http://192.168.59.105:8510/v1/kv/upstreams/base/192.168.59.103:20080

```

添加后的结果如下：

![](consul添加配置信息1.png)

![](consul添加配置信息2.png)

##### 2.3.5 启动nginx并查看运行情况

启动nginx服务：

```
$ sudo /opt/openresty/nginx/conf/nginx

```

##### 2.3.6 在redis中添加灰度发布的ip地址

```
$ redis-cli -h 127.0.0.1 -p 6379
127.0.0.1:6379> set 192.168.93.182 192.168.93.182

```

添加完成后，当192.168.93.182进行访问的时候，走灰度发布的链路，当其他ip进行访问的时候，走普通的线路。

#### 2.4 示例：根据IP地址匹配蓝绿发布的内容

在redis中添加ip地址，在consul配置前端的灰度发布的前端连接地址，通过不同的ip地址进行访问。

然后在不同的机器上使用selenium进行自动化访问测试，python脚本如下：

```python

# coding = utf-8
# 利用chromewebdriver模拟登陆系统，测试前端负载均衡效果
from selenium import webdriver
import time
#from time import sleep
from selenium.webdriver.chrome.options import Options
#from selenium.webdriver.firefox.options import Options

# 无头模式
# 无头模式不开启浏览器，也就是在程序里面运行的
chrome_options = Options()
#chrome_options.add_argument("--headless")
# 添加UserAgent
chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.190 Safari/537.36')
chrome_options.add_argument('--disable-gpu') 

chrome_options.add_argument('disable-infobars')     # 隐藏"Chrome正在受到自动软件的控制"
chrome_options.add_argument('lang=zh_CN.UTF-8')     # 设置中文
chrome_options.add_argument('window-size=1920x1080')     # 指定浏览器分辨率
chrome_options.add_argument('--hide-scrollbars')         # 隐藏滚动条, 应对一些特殊页面
chrome_options.add_argument('--remote-debugging-port=9222')

# #设置图片不加载
# prefs = {
#     'profile.default_content_setting_values': {
#         'images': 2
#     }
# }
# chrome_options.add_experimental_option('prefs', prefs)

browser = webdriver.Chrome(executable_path=(r'C:\Users\administrator\AppData\Local\Google\Chrome\Application\chromedriver.exe'), options=chrome_options)

# browser = webdriver.Firefox()

# 如果不用上面三行，那么就用下面这一行。运行的时候回自动的开启浏览器，并在浏览器中自动运行，你可以看到自动运行的过程
# browser = webdriver.Chrome(executable_path=(r'C:\Users\administrator\AppData\Local\Google\Chrome\Application\chromedriver.exe'))

# username_list = ["user002","user004","user005","user006","user007","user008","user009","user101","123","pangdezhen","xiaohong","lisa","lanwangji","weiwuxian","user20191126163930","user20191126164707","345"]

username_list = ["user002","user004","user008"]

for username in username_list:
    # 设置访问连接
    #browser.execute_script( 'window.open("http://192.168.59.105");' )
    browser.get("http://192.168.59.105")
    time.sleep(3)
    # 查找用户名、密码、验证码输入框
    browser.find_element_by_id("username").send_keys(username)
    browser.find_element_by_id("password").send_keys("123456")
    browser.find_element_by_id("checkCode").send_keys("1111")
    time.sleep(2)
    # 点击登录按钮进行登录(未完成)
    browser.find_element_by_class_name("login-button").click()
    time.sleep(7)
    print("=====用户已登录======")
    browser.find_element_by_class_name("anticon-user").click()
    time.sleep(1)
    browser.find_element_by_class_name("anticon-logout").click()
    time.sleep(1)
    browser.find_element_by_class_name("ant-btn-primary").click()
    print("=====用户已退出=======")
    # 重新加载页面
    browser.execute_script('window.location.reload("http://192.168.59.105")')
    time.sleep(1)
    browser.refresh()

time.sleep(5)
browser.close()
print("test end!!!")

```

最后执行这个脚本，查看服务中的日志信息。

## 前端发布问题

1. 前端cdn 307问题导致错误信息Uncaught SyntaxError: expected expression, got '<'
2. nginx配置，从不同位置加载不同的文件信息，可能存在加载混乱的情况，前端文件不会同时来自同一个前端服务
3. 无法精准控制用户访问哪个后向前端服务。
4. 目前只能做到对访问ip进行筛选和分析，尚未进一步完善灰度发布规则。
5. 前端尽可能的可配置，如何直接配置header参数而不是变更代码？

## 总结

终于他妈的写完文档了！

## 参考链接

前端发布部分：
* https://www.cnblogs.com/throwable/p/13113620.html
* https://www.jianshu.com/p/fadab3d092c5
* https://www.cnblogs.com/nf01/articles/13614279.html
* https://blog.csdn.net/zhaoydzhaoyd/article/details/107916968
* https://segmentfault.com/a/1190000014621318

后端发布部分：

* http://nepxion.gitee.io/docs/web-doc/Discovery%E3%80%90%E6%8E%A2%E7%B4%A2%E3%80%91%E5%BE%AE%E6%9C%8D%E5%8A%A1%E4%BC%81%E4%B8%9A%E7%BA%A7%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88.html
* https://github.com/Nepxion/Discovery