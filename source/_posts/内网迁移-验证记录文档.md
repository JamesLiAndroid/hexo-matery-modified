---
title: '内网迁移-: 验证记录文档'
top: false
cover: false
toc: true
mathjax: true
date: 2020-08-21 10:48:54
password:
summary:
tags:
categories:
---

## 内网环境地址准备

* 双网卡跳板机：192.168.81.90(Project-dev-01)、192.168.66.251（外网地址）、内外网交互

* 域名服务器：192.168.81.93(Project-dev-02)、内网

* 测试服务器：192.168.81.92(Project-dev-03)、内网

* 开发机：

## 搭建DNS域名解析服务器（使用代理的方式替代DNS服务器）

1. 在双网卡机器上挂载repo

2. 域名服务器配置仓库信息

3. 安装bind服务

4. 配置域名解析

## 原则上内网只能进不能出

## 内网的时钟同步的问题

首先在跳板机上搭建时钟同步服务，如下：

```
$ sudo yum install ntp

$ sudo vim /etc/ntp.conf
// 添加以下内容
restrict 192.168.81.0 mask 255.255.255.0 nomodify notrap
server 0.centos.pool.ntp.org iburst
server 1.centos.pool.ntp.org iburst
server 2.centos.pool.ntp.org iburst
server 3.centos.pool.ntp.org iburst
server cn.pool.ntp.org       iburst
server ntp1.aliyun.com       iburst
server 127.127.1.0
fudge 127.127.1.0 stratum 10


// :wq保存退出

// 启动服务
$ sudo systemctl enable ntpd
$ sudo systemctl start ntpd

// 查看服务运行情况
$ ntpq -p
     remote           refid      st t when poll reach   delay   offset  jitter
==============================================================================
+139.199.214.202 100.122.36.196   2 u   24   64  377   37.808   -1.303   6.896
xstratum2-1.ntp. 89.109.251.23    2 u   22   64  177  264.838  -69.270  24.058
-time.cloudflare 10.12.3.34       3 u   39   64    7  215.246   -1.959  16.837
+a.ams.pobot.net 17.253.34.253    2 u   18   64  377  341.399  -13.098  20.904
-ntp8.flashdance 192.36.143.150   2 u   79   64  356  306.375   30.427   7.068
*120.25.115.20   10.137.53.7      2 u   17   64  377   46.599    1.891   3.464
 localhost.local .INIT.          16 u    -  128    0    0.000    0.000   0.000

// 开启相应的端口信息
$ sudo firewall-cmd --add-port=123/udp --zone=public --permanent

$ sudo firewall-cmd --add-port=123/tcp --zone=public --permanent

$ sudo firewall-cmd --reload

```

跳板机上的服务就已经搭建成功了。下面要去设置内网机器的时钟同步，连接到跳板机！

对其它的内网机器进行设置：

```
$ sudo yum install ntp

$ sudo systemctl enable ntpd

$ sudo vim /etc/ntp.conf
// 修改以下内容指向192.168.81.90
restrict 192.168.81.90 nomodify notrap noquery

# Use public servers from the pool.ntp.org project.
# Please consider joining the pool (http://www.pool.ntp.org/join.html).
#server 0.centos.pool.ntp.org iburst
#server 1.centos.pool.ntp.org iburst
#server 2.centos.pool.ntp.org iburst
server 192.168.81.90 iburst

// :wq保存退出

$ sudo systemctl start ntpd

// 测试
$ sudo ntpdate -d 192.168.81.90
 3 Aug 19:35:56 ntpdate[29774]: ntpdate 4.2.6p5@1.2349-o Tue Jun 23 15:38:19 UTC 2020 (1)
Looking for host 192.168.81.90 and service ntp
host found : 192.168.81.90
transmit(192.168.81.90)
receive(192.168.81.90)
transmit(192.168.81.90)
receive(192.168.81.90)
transmit(192.168.81.90)
receive(192.168.81.90)
transmit(192.168.81.90)
receive(192.168.81.90)
server 192.168.81.90, port 123
stratum 3, precision -24, leap 00, trust 000
refid [192.168.81.90], delay 0.02583, dispersion 0.00006
transmitted 4, in filter 4
reference time:    e2d202b1.57312e3d  Mon, Aug  3 2020 11:27:45.340
originate timestamp: e2d20482.89df2f5c  Mon, Aug  3 2020 11:35:30.538
transmit timestamp:  e2d2752e.f2da3078  Mon, Aug  3 2020 19:36:14.948
filter delay:  0.02649  0.02591  0.02594  0.02583
         0.00000  0.00000  0.00000  0.00000
filter offset: -28844.4 -28844.4 -28844.4 -28844.4
         0.000000 0.000000 0.000000 0.000000
delay 0.02583, dispersion 0.00006
offset -28844.410413

 3 Aug 19:36:14 ntpdate[29774]: step time server 192.168.81.90 offset -28844.410413 sec

```

## 跳板机新增网卡的问题


## 以代理+host配置的方式，从跳板机设置对外访问的信息

前提：已经保证跳板机配置完成双网卡设置。

在跳板机，安装tinyproxy，如下：

```
$ sudo yum install tinyproxy

```

安装完成后进行配置，分为tinyproxy的配置以及要配置的出站域名信息，如下：

```
$ sudo vim /etc/tinyproxy/tinyproxy.conf

// 内容如下：
User tinyproxy
Group tinyproxy

Port 8888
// 注释掉bind部分
#Bind 192.168.0.1

Timeout 600
LogFile "/var/log/tinyproxy/tinyproxy.log"
LogLevel Info
PidFile "/var/run/tinyproxy/tinyproxy.pid"

// 注释掉Allow部分，允许所有代理设置后的服务器访问，不限制网段
#Allow 192.168.81.0/24

// 设置筛选的信息
Filter "/etc/tinyproxy/filter"
FilterURLs On
// 默认为Yes，如果是未指定URL信息，则不允许对外访问
FilterDefaultDeny Yes

// :wq保存退出

// 下面开始设置筛选URL的filter文件
$ sudo vim /etc/tinyproxy/filter
// 添加以下内容
aliyun.com
yum-idc.com
bit.edu.cn
163.com
docker.com
ustc.edu.cn
tsinghua.edu.com
lzu.edu.com
huaweicloud.com
bfsu.edu.cn
neusoft.edu.cn
cqu.edu.cn
fedoraproject.org
centos.org

pool.ntp.org
// :wq保存退出

```

配置完成后，启动tinyproxy。如下：

```
$ sudo systemctl enable tinyproxy
$ sudo systemctl start tinyproxy

// 开放端口信息
$ sudo firewall-cmd --add-port=8888/tcp --zone=public --permanent
$ sudo firewall-cmd --reload

```

跳板机配置完成后，转到内网机器上，配置代理信息，如下：

```
$ sudo vim /etc/profile
// 添加以下信息，注意：填写内网地址信息
export http_proxy=http://192.168.81.90:8888/
export FTP_PROXY=http://192.168.81.90:8888/
export ftp_proxy=http://192.168.81.90:8888/
export all_proxy=socks://192.168.81.90:8888/
export ALL_PROXY=socks://192.168.81.90:8888/
export HTTPS_PROXY=http://192.168.81.90:8888/
export https_proxy=http://192.168.81.90:8888/
export HTTP_PROXY=http://192.168.81.90:8888/
export no_proxy=localhost,127.0.0.1

// :wq保存退出

// 下面开始测试，用curl访问测试
$ curl --proxy 192.168.81.90:8888 docker.com

// 第二项，使用yum命令测试
$ sudo yum install epel-release
// 执行后输出大量错误日志

http://mirrors.163.com/centos/7.8.2003/os/x86_64/repodata/repomd.xml: [Errno 14] curl#6 - "Could not resolve host: mirrors.163.com; Unknown error"
Trying other mirror.
http://mirrors.ustc.edu.cn/centos/7.8.2003/os/x86_64/repodata/repomd.xml: [Errno 14] curl#6 - "Could not resolve host: mirrors.ustc.edu.cn; Unknown error"
Trying other mirror.
http://mirror.lzu.edu.cn/centos/7.8.2003/os/x86_64/repodata/repomd.xml: [Errno 14] curl#6 - "Could not resolve host: mirror.lzu.edu.cn; Unknown error"
Trying other mirror.
http://mirror.bit.edu.cn/centos/7.8.2003/os/x86_64/repodata/repomd.xml: [Errno 14] curl#6 - "Could not resolve host: mirror.bit.edu.cn; Unknown error"
Trying other mirror.
http://mirrors.nju.edu.cn/centos/7.8.2003/os/x86_64/repodata/repomd.xml: [Errno 14] curl#6 - "Could not resolve host: mirrors.nju.edu.cn; Unknown error"
Trying other mirror.
http://mirrors.neusoft.edu.cn/centos/7.8.2003/os/x86_64/repodata/repomd.xml: [Errno 14] curl#6 - "Could not resolve host: mirrors.neusoft.edu.cn; Unknown error"
Trying other mirror.
http://mirrors.bfsu.edu.cn/centos/7.8.2003/os/x86_64/repodata/repomd.xml: [Errno 14] curl#6 - "Could not resolve host: mirrors.bfsu.edu.cn; Unknown error"
Trying other mirror.
http://ftp.sjtu.edu.cn/centos/7.8.2003/os/x86_64/repodata/repomd.xml: [Errno 14] curl#6 - "Could not resolve host: ftp.sjtu.edu.cn; Unknown error"
Trying other mirror.
http://mirrors.tuna.tsinghua.edu.cn/centos/7.8.2003/os/x86_64/repodata/repomd.xml: [Errno 14] curl#6 - "Could not resolve host: mirrors.tuna.tsinghua.edu.cn; Unknown error"
Trying other mirror.
http://mirrors.cqu.edu.cn/CentOS/7.8.2003/os/x86_64/repodata/repomd.xml: [Errno 14] curl#6 - "Could not resolve host: mirrors.cqu.edu.cn; Unknown error"
Trying other mirror.
https://download.docker.com/linux/centos/7/x86_64/stable/repodata/repomd.xml: [Errno 14] curl#6 - "Could not resolve host: download.docker.com; Unknown error"
Trying other mirror.

// ctrl+c终止
// 修改yum代理信息
$ sudo vim /etc/yum.conf
// 最后进行追加
proxy=http://192.168.81.90:8888

// :wq保存退出

// 重新进行测试
$ sudo yum install epel-release

```
测试后上述两项都能正常返回数据，即为配置成功！

目前初步验证完成Linux下内网和外网通信的问题，使用tinyproxy在跳板机搭建代理程序，并配置对外访问的URL信息，在内网机器中设置代理信息，初步验证yum安装依赖、下载外网文件成功。但是这个存在比较大的风险：
1. 参考华为对内网管理的条例，要求文件只许进不许出，目前存在文件可以从内网拷贝出来的问题 ，需要了解咱公司对于内网的管理规定并商定解决方案
2. 目前对于跳板机的管理，配置可以对外访问的URL从跳板机上进行设定，而跳板机目前未进行权限管理，未限定登录人员，存在较大的风险。

希望就以上两个风险点，得到各位的建议和观点。目前处于验证阶段，只能优先抛出问题

## maven部分搭建

### maven构建与nexus仓库的导入导出

搭建nexus3依赖仓库。第一步，创建新的存储；第二步，创建新的镜像仓库，主要是针对hosted、proxy、group三种类型创建，其中proxy提供对外部的依赖仓库的访问，需要配置多个，hosted主要是存放本地开发的已打包的依赖信息，group是将上述两类镜像仓库合并到一起，统一由该仓库对外访问。第三步，创建新的角色信息，根据前面创建的镜像仓库，分配相应的访问权限到角色下，对应的权限中涉及的仓库信息，在下面进行设置。最后创建用户信息，分配前面对应的角色信息，即可完成对nexus3依赖仓库的设置。

在跳板机端配置代理域名，保证这些域名能够出站。切记，使用curl命令进行测试，不要使用ping命令测试，实践证明ping命令无法ping通域名，但是curl命令可以访问到各个出站地址的页面。

在本地开发机上，配置maven的settings.xml配置文件，执行示例程序的构建工作，设置如下：

settings.xml

```xml

<?xml version="1.0" encoding="UTF-8"?>

<!--
Licensed to the Apache Software Foundation (ASF) under one
or more contributor license agreements.  See the NOTICE file
distributed with this work for additional information
regarding copyright ownership.  The ASF licenses this file
to you under the Apache License, Version 2.0 (the
"License"); you may not use this file except in compliance
with the License.  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing,
software distributed under the License is distributed on an
"AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
KIND, either express or implied.  See the License for the
specific language governing permissions and limitations
under the License.
-->

<!--
 | This is the configuration file for Maven. It can be specified at two levels:
 |
 |  1. User Level. This settings.xml file provides configuration for a single user,
 |                 and is normally provided in ${user.home}/.m2/settings.xml.
 |
 |                 NOTE: This location can be overridden with the CLI option:
 |
 |                 -s /path/to/user/settings.xml
 |
 |  2. Global Level. This settings.xml file provides configuration for all Maven
 |                 users on a machine (assuming they're all using the same Maven
 |                 installation). It's normally provided in
 |                 ${maven.conf}/settings.xml.
 |
 |                 NOTE: This location can be overridden with the CLI option:
 |
 |                 -gs /path/to/global/settings.xml
 |
 | The sections in this sample file are intended to give you a running start at
 | getting the most out of your Maven installation. Where appropriate, the default
 | values (values used when the setting is not specified) are provided.
 |
 |-->
<settings xmlns="http://maven.apache.org/SETTINGS/1.0.0"
          xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
          xsi:schemaLocation="http://maven.apache.org/SETTINGS/1.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd">
  <!-- localRepository
   | The path to the local repository maven will use to store artifacts.
   |
   | Default: ${user.home}/.m2/repository
  <localRepository>/path/to/local/repo</localRepository>
  -->
  <localRepository>/home/centos/apache-maven-3.6.2/maven-repository</localRepository>
  <!-- interactiveMode
   | This will determine whether maven prompts you when it needs input. If set to false,
   | maven will use a sensible default value, perhaps based on some other setting, for
   | the parameter in question.
   |
   | Default: true
  <interactiveMode>true</interactiveMode>
  -->

  <!-- offline
   | Determines whether maven should attempt to connect to the network when executing a build.
   | This will have an effect on artifact downloads, artifact deployment, and others.
   |
   | Default: false
  <offline>false</offline>
  -->

  <!-- pluginGroups
   | This is a list of additional group identifiers that will be searched when resolving plugins by their prefix, i.e.
   | when invoking a command line like "mvn prefix:goal". Maven will automatically add the group identifiers
   | "org.apache.maven.plugins" and "org.codehaus.mojo" if these are not already contained in the list.
   |-->
  <pluginGroups>
    <!-- pluginGroup
     | Specifies a further group identifier to use for plugin lookup.
    <pluginGroup>com.your.plugins</pluginGroup>
    -->
  </pluginGroups>

  <!-- proxies
   | This is a list of proxies which can be used on this machine to connect to the network.
   | Unless otherwise specified (by system property or command-line switch), the first proxy
   | specification in this list marked as active will be used.
   |-->
  <proxies>
    <!-- proxy
     | Specification for one proxy, to be used in connecting to the network.
     |
    <proxy>
      <id>optional</id>
      <active>true</active>
      <protocol>http</protocol>
      <username>proxyuser</username>
      <password>proxypass</password>
      <host>proxy.host.net</host>
      <port>80</port>
      <nonProxyHosts>local.net|some.host.com</nonProxyHosts>
    </proxy>
    -->
  </proxies>

  <!-- servers
   | This is a list of authentication profiles, keyed by the server-id used within the system.
   | Authentication profiles can be used whenever maven must make a connection to a remote server.
   |-->
  <servers>
    <!-- server
     | Specifies the authentication information to use when connecting to a particular server, identified by
     | a unique name within the system (referred to by the 'id' attribute below).
     |
     | NOTE: You should either specify username/password OR privateKey/passphrase, since these pairings are
     |       used together.
     |
    <server>
      <id>deploymentRepo</id>
      <username>repouser</username>
      <password>repopwd</password>
    </server>
    -->

    <!-- Another sample, using keys to authenticate.
    <server>
      <id>siteServer</id>
      <privateKey>/path/to/private/key</privateKey>
      <passphrase>optional; leave empty if not used.</passphrase>
    </server>
    -->
    <server>
        <id>nexus-osc</id>
        <username>developer2</username>
        <password>hoteam2019</password>
    </server>
  </servers>
  <!-- mirrors
   | This is a list of mirrors to be used in downloading artifacts from remote repositories.
   |
   | It works like this: a POM may declare a repository to use in resolving certain artifacts.
   | However, this repository may have problems with heavy traffic at times, so people have mirrored
   | it to several places.
   |
   | That repository definition will have a unique id, so we can create a mirror reference for that
   | repository, to be used as an alternate download site. The mirror site will be the preferred
   | server for that repository.
   |-->
  <mirrors>
    <!-- mirror
     | Specifies a repository mirror site to use instead of a given repository. The repository that
     | this mirror serves has an ID that matches the mirrorOf element of this mirror. IDs are used
     | for inheritance and direct lookup purposes, and must be unique across the set of mirrors.
     |
    <mirror>
      <id>mirrorId</id>
      <mirrorOf>repositoryId</mirrorOf>
      <name>Human Readable Name for this Mirror.</name>
      <url>http://my.repository.com/repo/path</url>
    </mirror>
     -->
    <mirror>
      <id>nexus-osc</id>
      <mirrorOf>*</mirrorOf>
      <name>Nexus OSC</name>
      <url>http://192.168.81.94:8081/repository/maven-test-group/</url>
    </mirror>
  </mirrors>

  <!-- profiles
   | This is a list of profiles which can be activated in a variety of ways, and which can modify
   | the build process. Profiles provided in the settings.xml are intended to provide local machine-
   | specific paths and repository locations which allow the build to work in the local environment.
   |
   | For example, if you have an integration testing plugin - like cactus - that needs to know where
   | your Tomcat instance is installed, you can provide a variable here such that the variable is
   | dereferenced during the build process to configure the cactus plugin.
   |
   | As noted above, profiles can be activated in a variety of ways. One way - the activeProfiles
   | section of this document (settings.xml) - will be discussed later. Another way essentially
   | relies on the detection of a system property, either matching a particular value for the property,
   | or merely testing its existence. Profiles can also be activated by JDK version prefix, where a
   | value of '1.4' might activate a profile when the build is executed on a JDK version of '1.4.2_07'.
   | Finally, the list of active profiles can be specified directly from the command line.
   |
   | NOTE: For profiles defined in the settings.xml, you are restricted to specifying only artifact
   |       repositories, plugin repositories, and free-form properties to be used as configuration
   |       variables for plugins in the POM.
   |
   |-->
<!--  <profiles>  -->

    <!-- profile
     | Specifies a set of introductions to the build process, to be activated using one or more of the
     | mechanisms described above. For inheritance purposes, and to activate profiles via <activatedProfiles/>
     | or the command line, profiles have to have an ID that is unique.
     |
     | An encouraged best practice for profile identification is to use a consistent naming convention
     | for profiles, such as 'env-dev', 'env-test', 'env-production', 'user-jdcasey', 'user-brett', etc.
     | This will make it more intuitive to understand what the set of introduced profiles is attempting
     | to accomplish, particularly when you only have a list of profile id's for debug.
     |
     | This profile example uses the JDK version to trigger activation, and provides a JDK-specific repo.
    <profile>
      <id>jdk-1.4</id>

      <activation>
        <jdk>1.4</jdk>
      </activation>

      <repositories>
        <repository>
          <id>jdk14</id>
          <name>Repository for JDK 1.4 builds</name>
          <url>http://www.myhost.com/maven/jdk14</url>
          <layout>default</layout>
          <snapshotPolicy>always</snapshotPolicy>
        </repository>
      </repositories>
    </profile>
    -->

    <!--
     | Here is another profile, activated by the system property 'target-env' with a value of 'dev',
     | which provides a specific path to the Tomcat instance. To use this, your plugin configuration
     | might hypothetically look like:
     |
     | ...
     | <plugin>
     |   <groupId>org.myco.myplugins</groupId>
     |   <artifactId>myplugin</artifactId>
     |
     |   <configuration>
     |     <tomcatLocation>${tomcatPath}</tomcatLocation>
     |   </configuration>
     | </plugin>
     | ...
     |
     | NOTE: If you just wanted to inject this configuration whenever someone set 'target-env' to
     |       anything, you could just leave off the <value/> inside the activation-property.
     |
    <profile>
      <id>env-dev</id>

      <activation>
        <property>
          <name>target-env</name>
          <value>dev</value>
        </property>
      </activation>

      <properties>
        <tomcatPath>/path/to/tomcat/instance</tomcatPath>
      </properties>
    </profile>
    -->
  <activeProfiles>
    <activeProfile>nexus-osc</activeProfile>
  </activeProfiles>

  <profiles>
    <profile>
      <id>nexus-osc</id>
      <repositories>
        <repository>
          <id>central</id>
          <url>http://192.168.81.94:8081/repository/maven-test-group/</url>
          <releases><enabled>true</enabled></releases>
          <snapshots><enabled>true</enabled></snapshots>
        </repository>
      </repositories>
      <pluginRepositories>
        <pluginRepository>
          <id>central</id>
          <url>http://192.168.81.94:8081/repository/maven-test-group/</url>
          <releases><enabled>true</enabled></releases>
          <snapshots><enabled>true</enabled></snapshots>
        </pluginRepository>
      </pluginRepositories>
    </profile>
  </profiles>
<!--  </profiles> -->

  <!-- activeProfiles
   | List of profiles that are active for all builds.
   |
  <activeProfiles>
    <activeProfile>alwaysActiveProfile</activeProfile>
    <activeProfile>anotherAlwaysActiveProfile</activeProfile>
  </activeProfiles>
  -->
</settings>

```

测试项目构建，执行命令如下：

```
$ mvn clean install -U

```

初次构建时，会从nexus3中进行拉取相关的依赖信息，缓存在本地。如果当前依赖仓库中没有任何远程仓库的jar包，则会从配置的远程仓库中进行拉取操作。拉取完成后再缓存到本地，在后续构建中，就不会再去重复进行拉取了。

测试构建完成后，一定要测试一下构建和推送，执行命令如下:

```
$ mvn clean package deploy

```

这时候该构建的示例程序应该被推送到hosted类型的仓库中，前提是该仓库已经被设置为**allow redeploy**。

## 关于Linux内核的问题

kernel:BUG: soft lockup - CPU#2 stuck for 23s! [ksoftirqd/2:274]

内核软锁死，请检查磁盘使用率以及内存占用，估计内存也快跑满了

先尝试一下 
```
# echo 30 > /proc/sys/kernel/watchdog_thresh 
```
修改内核参数，让机器临时可以继续运行。

通过监控，看到后台cpu占用基本上吃满了。整体的实体机上cpu已经被吃满，导致其他虚拟机上同样出现了运行卡顿的情况！通过停止部分机器、释放资源的方式进行解决！


## jenkins设置

导入镜像并启动，同时把之前备份的稳定运行的镜像中/var/jenkins_home目录下的所有文件打包，命名为jenkins_home.tar.gz，传到目标机器上。

启动jenkins镜像，然后将jenkins_home.tar.gz拷贝到运行的容器中，删除容器下/var/jenkins_home目录下的所有文件，然后将jenkins_home.tar.gz解压，覆盖到/var/jenkins_home目录中。

最后重启jenkins容器，即可进行访问！

这样做最大限度保证了插件版本对应相应的jenkins版本，不会出现重装插件时导致jenkins中构建不可用的情况。

## gitlab设置

安装完成，添加用户，分组，创建项目即可

## EXSI系统中虚拟机假死，强制停机的操作

首先使用ssh连接到exsi中，操作如下：
```
$  ssh root@192.168.66.1
Password:

// 列出所有运行着的虚拟机信息
// 找到要关闭的机器信息
# esxcli vm process list
.....
....
Project-dev-03
   World ID: 2206934
   Process ID: 0
   VMX Cartel ID: 2206872
   UUID: 56 4d f1 af b7 b8 66 4f-02 29 3e 42 b1 d9 54 e8
   Display Name: Project-dev-03
   Config File: /vmfs/volumes/5d7280cc-085d5a77-2918-6c92bfdcfeba/Project-dev-03/Project-dev-03.vmx

// 记下上面的World ID信息
// 执行下面的命令进行关闭
// 注意：有三种关闭虚拟机的方法，Soft 程度最低，hard 为立即执行，如果依然不能关闭，则可以使用force 模式。
# esxcli vm process kill -t hard -w 2206934

// 重新执行前面的命令查看该机器是否还在运行
# esxcli vm process list
```

**注意：**一定要谨慎执行该命令！记得最后通过vsphere client查看VM状态。

## 为Docker挂载数据盘，解决存储冲突问题

目前遇到了xfs同docker中overlay2存储出现冲突的问题，错误如下：

```
Aug 01 17:35:28 localhost.localdomain systemd[1]: Received SIGRTMIN+20 from PID 288 (plymouthd).
Aug 01 17:35:28 localhost.localdomain kernel: XFS (dm-3): Metadata CRC error detected at xfs_allocbt_read_verify+0x1a/0xd0 [xfs], xfs_allocbt block
Aug 01 17:35:28 localhost.localdomain kernel: XFS (dm-3): Unmount and run xfs_repair
Aug 01 17:35:28 localhost.localdomain kernel: XFS (dm-3): First 128 bytes of corrupted metadata buffer:
Aug 01 17:35:28 localhost.localdomain kernel: ffff98b323926000: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00  ................
Aug 01 17:35:28 localhost.localdomain kernel: ffff98b323926010: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00  ................
Aug 01 17:35:28 localhost.localdomain kernel: ffff98b323926020: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00  ................
Aug 01 17:35:28 localhost.localdomain kernel: ffff98b323926030: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00  ................
Aug 01 17:35:28 localhost.localdomain kernel: ffff98b323926040: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00  ................
Aug 01 17:35:28 localhost.localdomain kernel: ffff98b323926050: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00  ................
Aug 01 17:35:28 localhost.localdomain kernel: ffff98b323926060: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00  ................
Aug 01 17:35:28 localhost.localdomain kernel: ffff98b323926070: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00  ................
Aug 01 17:35:28 localhost.localdomain kernel: XFS (dm-3): metadata I/O error in "xfs_trans_read_buf_map" at daddr 0x2a22f8 len 8 error 74
Aug 01 17:35:28 localhost.localdomain kernel: XFS (dm-3): xfs_do_force_shutdown(0x1) called from line 316 of file fs/xfs/xfs_trans_buf.c.  Return ad
Aug 01 17:35:28 localhost.localdomain kernel: XFS (dm-3): I/O Error Detected. Shutting down filesystem
Aug 01 17:35:28 localhost.localdomain kernel: XFS (dm-3): Please umount the filesystem and rectify the problem(s)
Aug 01 17:35:28 localhost.localdomain kernel: XFS (dm-3): Failed to recover intents
Aug 01 17:35:28 localhost.localdomain kernel: XFS (dm-3): log mount finish failed
Aug 01 17:35:28 localhost.localdomain mount[529]: mount: mount /dev/mapper/centos-root on /sysroot failed: Structure needs cleaning
Aug 01 17:35:28 localhost.localdomain systemd[1]: sysroot.mount mount process exited, code=exited status=32
Aug 01 17:35:28 localhost.localdomain systemd[1]: Failed to mount /sysroot.
Aug 01 17:35:28 localhost.localdomain systemd[1]: Dependency failed for Initrd Root File System.
Aug 01 17:35:28 localhost.localdomain systemd[1]: Dependency failed for Reload Configuration from the Real Root.
Aug 01 17:35:28 localhost.localdomain systemd[1]: Job initrd-parse-etc.service/start failed with result 'dependency'.
Aug 01 17:35:28 localhost.localdomain systemd[1]: Triggering OnFailure= dependencies of initrd-parse-etc.service.
Aug 01 17:35:28 localhost.localdomain systemd[1]: Job initrd-root-fs.target/start failed with result 'dependency'.
Aug 01 17:35:28 localhost.localdomain systemd[1]: Triggering OnFailure= dependencies of initrd-root-fs.target.
Aug 01 17:35:28 localhost.localdomain systemd[1]: Unit sysroot.mount entered failed state.
Aug 01 17:35:28 localhost.localdomain systemd[1]: Stopped dracut pre-udev hook.
Aug 01 17:35:28 localhost.localdomain systemd[1]: Stopped dracut cmdline hook.

```

按照[参考文章](https://www.jianshu.com/p/00ffd8df6010)中描述的内容，进行排查。但是当前机器已经不能正常开机，一旦开机只能进入紧急模式。所以需要考虑对硬盘进行修复。

再根据[参考文章](https://blog.csdn.net/runming56/article/details/81016404)进行修复，由于我们只有一个根目录存在的区，而且根目录所在区不能被umount。如果不能进行umount，是不能进行硬盘分区修复的。

尝试按照该[参考文章](https://blog.csdn.net/jycjyc/article/details/103073869)的方式进行修复，同样无效。

由于上述方式均无效，考虑对docker服务的存储单独挂载的手段解决这个问题。之所以这样考虑是因为目前的只有根目录的情况下，docker中的overlay2存储和centos本体的xfs存储存在冲突。那么单独对docker进行存储分区，使用ext4的方式进行存储，将可能出现的问题进行隔离！

### 1. 创建存储盘并挂载

```
// 查看当前硬盘情况
$ sudo fdisk -l

Disk /dev/sda: 128.8 GB, 128849018880 bytes, 251658240 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk label type: dos
Disk identifier: 0x000322d6

   Device Boot      Start         End      Blocks   Id  System
/dev/sda1   *        2048     2099199     1048576   83  Linux
/dev/sda2         2099200    41943039    19921920   8e  Linux LVM

Disk /dev/mapper/centos-root: 16.3 GB, 16299065344 bytes, 31834112 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 65536 bytes / 65536 bytes

// 开始分区
$ sudo fdisk /dev/sda
Welcome to fdisk (util-linux 2.23.2).

Changes will remain in memory only, until you decide to write them.
Be careful before using the write command.

// 创建新分区
Command (m for help): n
Partition type:
   p   primary (2 primary, 0 extended, 2 free)
   e   extended
Select (default p): p
Partition number (3,4, default 3): 3
Partition number (3,4, default 3): 3
First sector (41943040-251658239, default 41943040):
Using default value 41943040
// 确认新分区大小在50G
Last sector, +sectors or +size{K,M,G} (41943040-251658239, default 251658239): +50G
Partition 3 of type Linux and of size 50 GiB is set

// 保存设定
Command (m for help): w
The partition table has been altered!

Calling ioctl() to re-read partition table.

WARNING: Re-reading the partition table failed with error 16: Device or resource busy.
The kernel still uses the old table. The new table will be used at
the next reboot or after you run partprobe(8) or kpartx(8)
Syncing disks.

// 确认新的分区信息
$ sudo fdisk -l

Disk /dev/sda: 128.8 GB, 128849018880 bytes, 251658240 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk label type: dos
Disk identifier: 0x000322d6

   Device Boot      Start         End      Blocks   Id  System
/dev/sda1   *        2048     2099199     1048576   83  Linux
/dev/sda2         2099200    41943039    19921920   8e  Linux LVM
/dev/sda3        41943040   146800639    52428800   83  Linux

Disk /dev/mapper/centos-root: 16.3 GB, 16299065344 bytes, 31834112 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 65536 bytes / 65536 bytes

// 不重启机器使分区信息生效
$ sudo partprobe /dev/sda

// 创建ext4存储格式的分区
$ sudo mkfs.ext4 /dev/sda3
mke2fs 1.42.9 (28-Dec-2013)
Filesystem label=
OS type: Linux
Block size=4096 (log=2)
Fragment size=4096 (log=2)
Stride=0 blocks, Stripe width=0 blocks
3276800 inodes, 13107200 blocks
655360 blocks (5.00%) reserved for the super user
First data block=0
Maximum filesystem blocks=2162163712
400 block groups
32768 blocks per group, 32768 fragments per group
8192 inodes per group
Superblock backups stored on blocks:
        32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208,
        4096000, 7962624, 11239424

Allocating group tables: done
Writing inode tables: done
Creating journal (32768 blocks): done
Writing superblocks and filesystem accounting information: done

$ cd /mnt && sudo mkdir sda-docker
// 挂载新分区
$ sudo mount /dev/sda3 /mnt/sda-docker/
// 查看分区生效
$ df -h
Filesystem               Size  Used Avail Use% Mounted on
devtmpfs                 7.8G     0  7.8G   0% /dev
tmpfs                    7.8G     0  7.8G   0% /dev/shm
tmpfs                    7.8G  9.0M  7.8G   1% /run
tmpfs                    7.8G     0  7.8G   0% /sys/fs/cgroup
/dev/mapper/centos-root   16G  3.5G   12G  23% /
/dev/sda1               1014M  192M  823M  19% /boot
tmpfs                    1.6G     0  1.6G   0% /run/user/1000
/dev/sda3                 50G   53M   47G   1% /mnt/sda-docker

```

### 2. 备份docker的存储信息

```
$ sudo systemctl stop docker

$ sudo mv /var/lib/docker /var/lib/docker_data

$ sudo mkdir /var/lib/docker
// 将该文件夹/var/lib/docker的存储信息，放到/dev/sda3存储上
$ sudo mount /dev/sda3 /var/lib/docker

$ sudo mv /var/lib/docker_data/* /var/lib/docker/

$ sudo systemctl restart docker

// 验证一下是否真的存储到存储挂载的位置上了
$ cd /mnt/sda-docker/

$ sudo su

# ls 
builder  buildkit  containers  image  lost+found  network  overlay2  plugins  runtimes  swarm  tmp  trust  volumes

// 最后需要添加到开机后即进行挂载的情况
$ sudo vim /etc/rc.d/rc.local
// 在文件最后进行追加信息
echo "/dev/sda3 /var/lib/docker ext4 defaults 0 0" >>/etc/fstab
mount -a

// :wq保存退出

// 注意对该文件设定可执行权限，重启后才能生效
$ sudo chmod +x /etc/rc.d/rc.local

```

上述设置完成后，可以进行一次机器的重启，使所有内容生效！

需要查看docker中的相关文件时，可以先从/var/lib/docker路径下查看，也可以重新挂载/dev/sda3分区，如下：

```
$ sudo mount /dev/sda3 /mnt/sda-docker/

$ cd /mnt/sda-docker && ls -al

```

### 3. 总结

参考地址：
* https://www.cnblogs.com/sangmu/p/6629594.html
* https://www.jianshu.com/p/960535ba1318
* https://www.cnblogs.com/walter-xh/p/10590723.html


## 关于nexus3作为npm私有仓库的问题

1. 无法拉取代理的npm依赖包的问题

2. 关于现有使用docker构建时，无法使用yarn的情况

尝试分析nodejs的基础镜像，找到.npmrc文件，将nexus3的登录验证信息写入。以此重新构建基础镜像！

或者使用npm优先安装yarn，找到.yarnrc文件，将远程仓库的地址写入。

达成多阶段构建要素！

3. nexus3配置匿名访问的注意事项

![](匿名访问.png)

## 内网环境下对于npm和yarn的设定

在服务器中全局代理设置完成的背景下，设置npm工具和yarn的配置。

首先是npm的配置文件**.npmrc*文件：

```.npmrc

registry=http://192.168.81.94:8081/repository/npm-test-group-all/
email=npm-test@hoteamsoft.com
always-auth=true
strict-ssl=false
//192.168.81.94:8081/repository/npm-test-group-all/:_authToken=NpmToken.5a59b901-9df2-3489-8c6b-78d0f5ac3346
sass-binary-site=http://192.168.81.94:8081/repository/npm-test-proxy-node-sass/

```

具体设置方式:

```
$ npm config set registry http://192.168.81.94:8081/repository/npm-test-group-all/
$ npm config set sass-binary-site http://192.168.81.94:8081/repository/npm-test-proxy-node-sass/
$ npm config set always-auth true
$ npm config set strict-ssl=false
$ npm login
username: npm-test
password:
Email: npm-test@hoteamsoft.com



```

然后是**.yarnrc*文件：

```.yarnrc
# THIS IS AN AUTOGENERATED FILE. DO NOT EDIT THIS FILE DIRECTLY.
# yarn lockfile v1


registry "http://192.168.81.94:8081/repository/npm-test-group-all/"
email npm-test@hoteamsoft.com
lastUpdateCheck 1596709896121
strict-ssl false
username npm-user
sass_binary_site http://192.168.81.94:8081/repository/npm-test-proxy-node-sass/

```

具体设置方式：

```
$ yarn config set registry http://192.168.81.94:8081/repository/npm-test-group-all/
$ yarn config set sass_binary_site http://192.168.81.94:8081/repository/npm-test-proxy-node-sass/
$ yarn config set always-auth true
$ yarn config set strict-ssl=false

```

## 关于前端构建的问题

1. node-sass安装问题

利用离线的方式安装node-sass，区分在Linux下的构建和在windows构建的不同！

对package.json改造，修改preinstall选项，以及添加指定nodeSass的配置项，主要是binding-node的选项！

```
npm install node-sass --sass_binary_path=D:\\node_modules\\win32-x64-64_binding.node

```

貌似上述安装方式并不能对yarn起作用！

只能是学习Linux中的安装方式，安装python2，尝试自己编译node-sass需要的依赖信息！

最终解决方式：

利用nexus3代理仓库，配置对node-sass的代理，将淘宝镜像'https://npm.taobao.org/mirrors/node-sass/'地址进行代理，如下：

![](node-sass代理.jpg)

然后针对node-sass在windows侧本地的安装，配置node-sass在nexus3中的代理地址。在**.npmrc*或者**.yarnrc*中设置，如下：

```
// 追加下面的配置
sass-binary-site=http://192.168.81.94:8081/repository/npm-test-proxy-node-sass/

```

或者在命令行中配置

```

（Linux） export SASS_BINARY_SITE=http://192.168.81.94:8081/repository/npm-test-proxy-node-sass/

（windows） set SASS_BINARY_SITE=http://192.168.81.94:8081/repository/npm-test-proxy-node-sass/

```

如果SASS_BINARY_SITE无法访问，需要自行下载binding.node文件，然后将该文件所在路径设置为SASS_BINARY_PATH，如下：

```

（Linux） export SASS_BINARY_PATH=~/node_modules/win32-x64-64.binding.node

（windows） set SASS_BINARY_PATH=D:\node_modules\win32-x64-64.binding.node

```

最后，安装node-sass。

```
$ npm install -g node-sass -D --verbose

```

**针对yarn的设置**，经常出现下载不到binding.node文件，下载不到node-headers.tar.gz的问题

经过长期验证，得出以下的步骤：

1.1 找到yarn的缓存位置

```
$ yarn cache dir
/home/centos/.cache/yarn/v6（Linux路径）
D:\node_modules\cache\Yarn\Cache\v6

```

1.2 找到node-sass所在的路径，这里使用Everything查找yarn的路径，找到名称为：**npm-node-sass-4.14.1-xxxxxxxxxxxx-integrity**文件夹。要注意该文件夹可能有多个，需要定位到上面找到的yarn缓存路径！

1.3 查看你需要的binding.node文件版本，需要对package.json进行改造，如下：

```
// 在scripts下添加preinstall相关的执行命令
"scripts": {
        "preinstall": "node -p \"[process.platform, process.arch, process.versions.modules].join('-')\" ",
        ......
    },

```

添加后再执行

```
$ npm install
// 输出下面的信息
win32-x64-64

```

1.3 进入到上面找到的node-sass缓存路径，到[github地址](https://github.com/sass/node-sass/releases)对应你系统版本的binding.node文件，找到**win32-x64-64_binding.node**，下载下来。

1.4 将上面下载的文件copy到上面的yarn缓存路径下的**npm-node-sass-4.14.1-xxxxxxxxxxxx-integrity**文件夹中。在该文件夹中创建:

```
$ mkdir -p vendor/win32-x64-64

```

然后将**win32-x64-64_binding.node**改名为**binding.node**，放到刚才创建的文件夹中，放到win32-x64-64之中！

1.5 最后执行

```
$ yarn install --network-timeout=1000000 --verbose

```

参考文章：

* https://www.jianshu.com/p/8aab08de6243

* https://www.jianshu.com/p/947d050f98f2

* https://www.jianshu.com/p/24d2b118a3ce

2. 代理仓库nexus3中依赖拉取的问题

检查Linux服务器上关于代理转发的问题，配置/etc/profile中的代理信息，前文已经阐述了。

剩下的可以直接在命令行中设置。

3. 关于windows端代理的设置

使用proxifier进行全局代理的设置，需要注意的是，启动该软件时必须使用管理员权限。否则无法进行代理操作！

除了全局代理之外，剩下的可以在命令行中使用set命令设置，如下：

```
$ set http_proxy=http://192.168.81.90:8888

$ set https_proxy=http://192.168.81.90:8888

```

4. 在jenkins中进行前端构建，解决node-sass无法下载的问题

在前端构建过程中，由于全程使用docker进行构建，导致我们无法像在windows开发机上那样，手动拷贝binding.node文件到缓存文件夹下的路径中。这样就必须更换其他方式。

之前在nexus3设置中，使用了maven类型的仓库来代理淘宝的node-sass中的dist镜像，后来发现这样做不正确，重新使用raw类型的仓库来代替，创建如下图的仓库信息：

![](raw类型的dist代理仓库.jpg)

仓库创建完成后，下一步需要进行配置，在Dockerfile中配置node-sass依赖信息，在**yarn install**指令执行之前，添加node-sass本地仓库信息，如下：

```
...
// 添加下面的内容
yarn install --verbose --network-timeout=1000000 --sass_binary_site=http://192.168.81.94:8081/repository/raw-proxy-taobao-dist/node-sass
...

```

这样设置完成后，触发jenkins的构建即可，后续看到可以正常拉取binding.node文件，这样就可以正常进行构建了！

构建完成后，是可以看到nexus3中已经拉取了相应的依赖信息，如下图：

![](获取的node-sass缓存到nexus3中.jpg)

最后，附上完整的Dockerfile文件，如下：

```Dockerfile

# 第一层面构建，打包前端代码
#### 1. 指定node镜像版本
FROM 192.168.81.94:5000/node:10.16.0 AS builder

# 添加日期信息，如果需要更新缓存层，更新该处日期信息即可
ENV REFRESH_DATE 2020-08-20_10:11

# 2. 指定编译的工作空间
WORKDIR /home/node/app

# 3. 设置授权信息，本地开发机登录后获取私有镜像的authToken信息，放入.npmrc
RUN touch ~/.npmrc && mkdir ~/node_global && mkdir ~/node_cache && echo prefix=~/node_global \n\
cache=~/node_cache \n\
registry=http://192.168.81.94:8081/repository/npm-test-group-all/ \n\
//192.168.81.94:8081/repository/npm-test-group-all/:_authToken=NpmToken.5a59b901-9df2-3489-8c6b-78d0f5ac3346

# 4. 构建之前清理缓存
# RUN npm cache clean --force

# COPY .npmrc /home/node/app
# COPY .yarnrc /home/node/app

# 4. 安装打包需要的yarn工具
RUN npm --registry http://192.168.81.94:8081/repository/npm-test-group-all/ install -g yarn

# 对yarn设置淘宝镜像
RUN yarn config set registry http://192.168.81.94:8081/repository/npm-test-group-all/
#RUN yarn config set sass_binary_site http://192.168.81.94:8081/repository/raw-proxy-taobao-dist/node-sass

RUN mkdir .cache && yarn config set cache-folder /home/node/app/.cache/

# 4. 添加package.json
COPY package.json /home/node/app/
#COPY ./vendor/ /home/node/app/

#ENV SASS_BINARTY_PATH /home/node/app/vendor/linux-x64-64_binding.node

# 安装依赖信息
# 注意这句，在后面跟上sass_binary_site的标记，指定从本地代理拉取binding.node文件！
RUN yarn install --verbose --network-timeout=1000000 --sass_binary_site=http://192.168.81.94:8081/repository/raw-proxy-taobao-dist/node-sass

# 6. 添加剩余代码到工作空间
COPY . /home/node/app
# 7. 编译代码
RUN yarn run build

# 第二层面构建
#### 1.拉取自定义镜像名称
FROM 192.168.81.94:5000/base_frontend:0.0.1

# 2.将打包后的代码复制到运行位置
COPY --from=builder /home/node/app/dist /var/www

# 3.启动nginx
ENTRYPOINT ["nginx","-g","daemon off;"]

```

参考地址：

* https://blog.csdn.net/a910196454/article/details/106220132